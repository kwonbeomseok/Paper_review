#FleXOR
##FleXOR: Trainable Fractional Quantization
paper link
facebook link https://www.facebook.com/groups/TensorFlowKR/permalink/1309523079388747

* 알고리즘
- only weight quantization, activation is not quantized
- non-uniform precision quantization
- backward시 tanh, scale factor를 이용한 방법 사용


#### 다른 논문과의 비교 방식

#### 흥미로운 점

#### 의문점
- XOR-Gate 1개를 공유해서 사용한다고 했는데 가능한건지
- XOR-Gate의 조합은 어떻게 결정되는지

#### 찾아볼 개념, 논문
- Hamming distance
- XOR-gate 이전 논문 볼 필요 있을 듯
